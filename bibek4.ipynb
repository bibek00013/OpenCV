{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d106cc06",
   "metadata": {},
   "source": [
    "Image data generator -> Loading image from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a39cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D ,MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832559ba",
   "metadata": {},
   "source": [
    "Create image data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b117651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255 #normalize to the range of 0-1 to make better computation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ea110",
   "metadata": {},
   "source": [
    "Loading images from the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd8cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_datagen.flow_from_directory(\n",
    "     \"dataset/train\",\n",
    "     target_size = (128 ,128), #make images of same size\n",
    "     batch_size = 32, #32 images are processed at once\n",
    "     class_mode = \"binary\"\n",
    ") \n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    \"dataset/test\",\n",
    "    target_size = (128 ,128),\n",
    "    batch_size = 32,\n",
    "    class_mode = \"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90ebdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a488e",
   "metadata": {},
   "source": [
    "Building the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ae5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhair\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Dropout.__init__() missing 1 required positional argument: 'rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m model = Sequential([\n\u001b[32m      2\u001b[39m     Conv2D(\u001b[32m32\u001b[39m,(\u001b[32m3\u001b[39m,\u001b[32m3\u001b[39m), activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m, input_shape = (\u001b[32m128\u001b[39m ,\u001b[32m128\u001b[39m ,\u001b[32m3\u001b[39m)),\n\u001b[32m      3\u001b[39m     MaxPooling2D(\u001b[32m2\u001b[39m,\u001b[32m2\u001b[39m),\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     Conv2D(\u001b[32m64\u001b[39m, (\u001b[32m3\u001b[39m,\u001b[32m3\u001b[39m), activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m     MaxPooling2D(\u001b[32m2\u001b[39m,\u001b[32m2\u001b[39m),\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m     Flatten(),\n\u001b[32m      9\u001b[39m     Dense(\u001b[32m64\u001b[39m , activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     10\u001b[39m     Dense(\u001b[32m1\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m ])\n",
      "\u001b[31mTypeError\u001b[39m: Dropout.__init__() missing 1 required positional argument: 'rate'"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32,(3,3), activation=\"relu\", input_shape = (128 ,128 ,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64 , activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\"),\n",
    "    Dropout(0.5)  #disables 50 neuron randomly\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b1dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\", #The optimizer decides how the model updates its weights to reduce errors.\n",
    "    loss= \"binary_crossentropy\", #Loss measures how wrong the model’s prediction is.\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a425c",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d200342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.7322 - val_accuracy: 0.5000 - val_loss: 0.6113\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.5000 - loss: 0.6113 - val_accuracy: 1.0000 - val_loss: 0.5123\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 0.5123 - val_accuracy: 1.0000 - val_loss: 0.4046\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 1.0000 - loss: 0.4046 - val_accuracy: 1.0000 - val_loss: 0.2139\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 1.0000 - loss: 0.2139 - val_accuracy: 1.0000 - val_loss: 0.0765\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 0.0765 - val_accuracy: 1.0000 - val_loss: 0.0226\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 0.0226 - val_accuracy: 1.0000 - val_loss: 0.0254\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.0254 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 9.6263e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 9.6263e-04 - val_accuracy: 1.0000 - val_loss: 1.8797e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13e7d33a270>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    epochs= 10, #the model learns from the full dataset 10 times.\n",
    "    validation_data= test_data\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658b5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img(\"dataset/test/cat/img1.jpg\", target_size=(128,128)) #loads the image\n",
    "img = image.img_to_array(img) / 255 # image converted into array because  of CNN model \n",
    "img = np.expand_dims(img ,axis=0) #converts image shape into batchsize because model predicts in batchsize\n",
    "prediction = model.predict(img) \n",
    "\n",
    "if prediction[0]>0.5:\n",
    "    print(\"Dog\")\n",
    "else:\n",
    "    print(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbee8e",
   "metadata": {},
   "source": [
    "Data Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249adb4f",
   "metadata": {},
   "source": [
    "Data augmentation creates slightly modified versions of your images while training.\n",
    "\n",
    "Flipping, rotating, zooming, shifting, etc.\n",
    "\n",
    "Increases effective dataset size\n",
    "\n",
    "Helps the model learn general patterns instead of memorizing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a462842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range =0.2,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83027ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n",
      "Found 2 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_datagen.flow_from_directory(\n",
    "    \"dataset/train\",\n",
    "    target_size = (128,128),\n",
    "    batch_size = 32,\n",
    "    class_mode = \"binary\"\n",
    "\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    \"dataset/test\",\n",
    "    target_size = (128,128),\n",
    "    batch_size = 32,\n",
    "    class_mode = \"binary\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
